{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# default_exp core"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "################################################################################\n",
      "### WARNING, path does not exist: KALDI_ROOT=/mnt/matylda5/iveselyk/Tools/kaldi-trunk\n",
      "###          (please add 'export KALDI_ROOT=<your_path>' in your $HOME/.profile)\n",
      "###          (or run as: KALDI_ROOT=<your_path> python <your_script>.py)\n",
      "################################################################################\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#export\n",
    "from dreamai2.learner import*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%reload_ext autoreload"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def add_to_group(img, anchor, sim_dict, group_dict):\n",
    "    '''Add image to group\n",
    "\n",
    "    Args:\n",
    "        img (str)\n",
    "        anchor (str)\n",
    "        sim_dict (dict)\n",
    "        group_dict (dict)\n",
    "    '''\n",
    "    sim_dict[img]['has_group'] = True\n",
    "    group_dict[anchor].append(img)\n",
    "\n",
    "def batchify_data(imgs, anchors, image_size=1024):\n",
    "    '''Convert images lists to batches\n",
    "\n",
    "    Args:\n",
    "        imgs ([type]): List of images\n",
    "        anchors ([type]): List of anchors\n",
    "        image_size (int, optional): Defaults to 1024.\n",
    "    '''\n",
    "    img_batch = to_batch(paths=imgs, size=(image_size,image_size)).to(device)\n",
    "    anchor_batch = to_batch(paths=anchors, size=(image_size,image_size)).to(device)\n",
    "\n",
    "def byol_model(model_path=''):\n",
    "    '''Load the trained byol model\n",
    "\n",
    "    Args:\n",
    "        model_path (str, optional): Pretrained state_dict path. Defaults to ''.\n",
    "    '''\n",
    "    \n",
    "    model = create_body(resnet18, cut=-1, num_extra=0)\n",
    "    if len(model_path) == 0  and os.path.exists(model_path):\n",
    "        sd = torch.load('rnet18_byol.pt', map_location='cpu')\n",
    "        sd = swap_dict_key_letters(sd, 'conv1.weight', '0.model.0.weight', strict=True)\n",
    "        keys = ['bn1.weight', 'bn1.bias', 'bn1.running_mean', 'bn1.running_var', 'bn1.num_batches_tracked',]\n",
    "        for k in keys:\n",
    "            sd = swap_dict_key_letters(sd, k, f'0.model.1.{k.split(\".\")[-1]}', strict=True)\n",
    "        for i in range(1,5):\n",
    "            sd = swap_dict_key_letters(sd, f'layer{i}', f'0.model.{i+3}')\n",
    "        model.load_state_dict(sd, strict=False)\n",
    "    return model\n",
    "\n",
    "def create_groups(imgs, anchors, model_path='', image_size=1024, bs=6, thresh=0.99, device='cpu'):\n",
    "    '''Create n groups where n is the number of anchor images.\n",
    "\n",
    "    Args:\n",
    "        imgs (list): List of images\n",
    "        anchors (list): List of anchor images\n",
    "        model_path (str, optional): Pretrained state_dict path. Defaults to ''.\n",
    "        image_size (int, optional): Defaults to 1024.\n",
    "        bs (int, optional): Batch size. Defaults to 6.\n",
    "        thresh (float, optional): Similarity threshold. Defaults to 0.99.\n",
    "        device (str, optional): Defaults to 'cpu'.\n",
    "    '''\n",
    "    \n",
    "    if not is_str(imgs[0]):\n",
    "        imgs = [str(img) for img in imgs]\n",
    "        \n",
    "    if not is_str(anchors[0]):\n",
    "        anchors = [str( anchor) for anchor in anchors]\n",
    "    \n",
    "    model = byol_model(model_path=model_path)\n",
    "    device = torch.device(device)\n",
    "    model.eval().to(device)\n",
    "    \n",
    "    anchor_df = pd.DataFrame({'Anchors':anchors})    \n",
    "    img_df = pd.DataFrame({'Images':imgs})\n",
    "    anchor_ds = DaiDataset(data=anchor_df, tfms=instant_tfms(h=image_size, w=image_size, test_tfms=False))\n",
    "    img_ds = DaiDataset(data=img_df, tfms=instant_tfms(h=image_size, w=image_size, test_tfms=False))\n",
    "    anchor_dl = DataLoader(anchor_ds, batch_size=bs, shuffle=False, num_workers=4, pin_memory=True)\n",
    "    img_dl = DataLoader(img_ds, batch_size=bs, shuffle=False, num_workers=4, pin_memory=True)\n",
    "    \n",
    "    anchor_vectors = []\n",
    "    img_vectors = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for anchor_batch in anchor_dl:\n",
    "            anchor_vectors.append(model(anchor_batch['x'][0].to(device)).flatten(start_dim=1))\n",
    "        for img_batch in img_dl:\n",
    "            img_vectors.append(model(img_batch['x'][0].to(device)).flatten(start_dim=1))\n",
    "    anchor_vectors = torch.cat(anchor_vectors)\n",
    "    img_vectors = torch.cat(img_vectors)\n",
    "    anchor_vectors = [x.repeat(len(img_vectors),1) for x in anchor_vectors]\n",
    "    \n",
    "    cos = nn.CosineSimilarity(dim=1, eps=1e-6)\n",
    "    sims = [cos(x, img_vectors) for x in anchor_vectors]\n",
    "    \n",
    "    sim_dict = {img:{'has_group':False, 'best':0, 'best_anchor':''} for img in imgs}\n",
    "    group_dict = {anchor:[] for anchor in anchors}\n",
    "    for i,anchor in enumerate(anchors):\n",
    "        for j,img in enumerate(imgs):\n",
    "            sim = sims[i][j]\n",
    "            if sim >= sim_dict[img]['best']:\n",
    "                sim_dict[img]['best'] = sim.detach().item()\n",
    "                sim_dict[img]['best_anchor'] = anchor\n",
    "                if sim >= thresh:\n",
    "                    add_to_group(img, anchor, sim_dict, group_dict)\n",
    "            if i == len(anchors)-1 and not sim_dict[img]['has_group']:\n",
    "                add_to_group(img, sim_dict[img]['best_anchor'], sim_dict, group_dict)\n",
    "    return group_dict\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n"
     ]
    }
   ],
   "source": [
    "# #cuda\n",
    "\n",
    "# data_name = 'final_demo'\n",
    "# data_path = Path(f'/media/hamza/data2/{data_name}/')\n",
    "# images_path = data_path/'zoey_synced'\n",
    "\n",
    "# all_imgs = sorted_paths(images_path, key=path_name, make_str=True)\n",
    "# anchors = [all_imgs[i] for i in range(0,len(all_imgs),3)]\n",
    "# imgs = flatten_list([all_imgs[i+1:i+3] for i in range(0,len(all_imgs),3)])\n",
    "\n",
    "# device = default_device()\n",
    "\n",
    "# group_dict = create_groups(imgs, anchors, model_path='rnet18_byol.pt', image_size=1024, bs=3, device=device)\n",
    "\n",
    "# assert sorted(dict_keys(group_dict)) == sorted(anchors)\n",
    "\n",
    "# assert sorted(dict_keys(create_groups(imgs, anchors=[Path(a) for a in anchors], model_path='', image_size=1024, bs=3, device=device))) == sorted(anchors)\n",
    "\n",
    "# assert len(flatten_list(dict_values(group_dict))) == len(imgs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.10 64-bit ('dreamai': conda)",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
